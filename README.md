# ğŸ›¡ï¸ Veille Technologique â€“ CybersÃ©curitÃ© en IA

## ğŸ‘¤ Auteur
*DAZAYOUS Tim*  
DÃ©veloppeur junior â€“ Projet SIDORA IA

## ğŸ“Œ Objectifs de la veille
- Comprendre les enjeux de cybersÃ©curitÃ© appliquÃ©s Ã  lâ€™intelligence artificielle.  
- Mettre en place un rituel de veille technologique automatisÃ© et rÃ©gulier.  
- Identifier les sources fiables et pertinentes.  
- Produire une restitution structurÃ©e et comprÃ©hensible pour les parties prenantes.

## âš™ï¸ Outils utilisÃ©s
- **Feedly** : agrÃ©gateur de flux RSS  
- **Google Alerts** : alertes automatiques par email  
- **Pocket** : sauvegarde et annotation dâ€™articles

## ğŸ”— Sources RSS suivies

### Flux RSS officiels suivis via Feedly
- Microsoft Security Blog : [https://www.microsoft.com/en-us/security/blog/](https://www.microsoft.com/en-us/security/blog/)  
- Google AI Blog : [https://ai.googleblog.com/](https://ai.googleblog.com/)  
- OpenAI Blog : [https://openai.com/blog](https://openai.com/blog)  
- arXiv Machine Learning (cs.LG) : [https://export.arxiv.org/rss/cs.LG](https://export.arxiv.org/rss/cs.LG)  
- arXiv Artificial Intelligence (cs.AI) : [https://export.arxiv.org/rss/cs.AI](https://export.arxiv.org/rss/cs.AI)  
- The Hacker News : [https://feeds.feedburner.com/TheHackersNews](https://feeds.feedburner.com/TheHackersNews)  

### Page suivie via Feedly (pas de RSS officiel)
- OWASP ML Security Top 10 : [https://owasp.org/www-project-machine-learning-security-top-10/](https://owasp.org/www-project-machine-learning-security-top-10/)

## :loudspeaker: Alertes Google paramÃ©trÃ©es

### Surveillance en temps rÃ©el mise en place sur plusieurs domaines
* Cyber attaque IA
* Cyber attack AI
* CybersecuritÃ© IA
* Cybersecurity AI

## ğŸ•’ Rituel de veille automatisÃ©
- **Lundi** : ouverture de Feedly â†’ lecture des nouveautÃ©s dans le dossier "CybersÃ©curitÃ© IA"  
- **Mercredi** : tri des articles â†’ sauvegarde des plus pertinents dans Pocket  
- **Weekend** : mise Ã  jour de la synthÃ¨se dans ce fichier README.md

## ğŸ´â€â˜ ï¸ Principales menaces
- **Attaques adversariales** : modification subtile des entrÃ©es pour tromper le modÃ¨le  
- **Poisoning des donnÃ©es** : altÃ©ration du dataset dâ€™entraÃ®nement  
- **Vol ou extraction de modÃ¨les** : rÃ©cupÃ©ration dâ€™un modÃ¨le via ses rÃ©ponses  
- **Backdoors dans les modÃ¨les** : comportements cachÃ©s dÃ©clenchÃ©s par des motifs spÃ©cifiques  
- **Prompt injection (LLM)** : manipulation des modÃ¨les de langage pour obtenir des donnÃ©es sensibles  
- **Risques de supply chain IA** : dÃ©pendances logicielles corrompues ou malveillantes

## ğŸ” Bonnes pratiques pour se protÃ©ger
- VÃ©rification et nettoyage des donnÃ©es dâ€™entraÃ®nement  
- Tests adversariaux rÃ©guliers  
- Filtrage des prompts pour les LLM  
- Mise Ã  jour et gestion sÃ©curisÃ©e des dÃ©pendances  
- Application des cadres de sÃ©curitÃ© : NIST AI RMF, normes OWASP

> Exemple d'installation d'outil pour des tests de securitÃ© IA
```bash
pip install secml
pip install adversarial-robustness-toolbox
```
> Il existe des bibliothÃ¨ques pour tester la robustesse d'un modÃ¨le IA face Ã  des attaques.
```python
from art.attacks.evasion import FastGradientMethod
from art.estimators.classification import SklearnClassifier

classifier = SklearnClassifier(model=mon_modele)
attack = FastGradientMethod(estimator=classifier)
x_adv = attack.generate(x=test_imgs)
```

## ğŸ“… SynthÃ¨se hebdomadaire

### Semaine 1
- Flux RSS configurÃ©s et testÃ©s  
- Premiers articles lus et sauvegardÃ©s  

### Semaine 2
- Analyse des articles les plus pertinents  
- Mise Ã  jour du README.md avec nouvelles informations
- Alertes Googles definies

### Semaine 3
- Mise Ã  jour du README.md
- CrÃ©ation d'un google slide reprenants les principaux points vu lors de la veille

## ğŸ¯ Conclusion
Cette veille technologique m'a permis :
- de comprendre d'avantage les risques liÃ©s Ã  lâ€™IA,  
- de mettre en place un rituel automatisÃ© de suivi des sources "fiables"  
- de commencer Ã  sÃ©lectionner des outils et pratiques pour sÃ©curiser les projets IA.

